{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignmnet 1_LI XINYA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import randn\n",
    "import torch\n",
    "from torch import nn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. Implement the neural network in pytorch\n",
    "\n",
    "1. Input (x1,x2): 2 nodes\n",
    "2. First hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation function\n",
    "3. Second hidden layer: 10 nodes, with weights (w) and bias (b), sigmoid activation function\n",
    "4. Output (predict): 1 node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (1): Sigmoid()\n",
      "  (2): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (3): Sigmoid()\n",
      "  (4): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_shape = 2\n",
    "output_shape = 1\n",
    "\n",
    "#define the neural network model\n",
    "network_model = torch.nn.Sequential(\n",
    "    # define input layer\n",
    "    torch.nn.Linear(input_shape, 10),\n",
    "    nn.Sigmoid(),\n",
    "    # define first hidden layer\n",
    "    torch.nn.Linear(10, 10),\n",
    "    nn.Sigmoid(),\n",
    "    # define second hidden layer\n",
    "    torch.nn.Linear(10, output_shape)\n",
    ")\n",
    "\n",
    "print(network_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. Generate the input date\n",
    "(x1,x2) \\in [0,1] drawn from a uniform random distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9274 0.6714] tensor([0.9274, 0.6714])\n"
     ]
    }
   ],
   "source": [
    "x1 = np.random.uniform(0,1)\n",
    "x2 = np.random.uniform(0,1)\n",
    "x_input = torch.tensor([x1,x2])\n",
    "x = np.array([round(x1,4),round(x2,4)])\n",
    "print(x,x_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Generate the labels y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = torch.tensor((x1*x1+x2*x2)/2)\n",
    "y = np.array([(x1*x1+x2*x2)/2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Implement a loss function L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_pred, y_label):\n",
    "    return torch.sum((y_pred-y_label)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5. Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.43650704622268677\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(network_model.parameters(), lr=0.1)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "y_pred = network_model(x_input)\n",
    "loss = loss_function(y_pred, y_label)\n",
    "loss.backward()\n",
    "\n",
    "print(\"Loss = \" + str(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6. Compute the gradients using pytorch autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = network_model[0]\n",
    "hidden_layer1 = network_model[2]\n",
    "hidden_layer2 = network_model[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.4365\n",
      "Input layer weight gradient: [[-0.0116 -0.0084]\n",
      " [-0.0023 -0.0016]\n",
      " [ 0.0071  0.0052]\n",
      " [ 0.0058  0.0042]\n",
      " [ 0.0057  0.0041]\n",
      " [-0.0028 -0.002 ]\n",
      " [-0.0103 -0.0074]\n",
      " [ 0.0069  0.005 ]\n",
      " [-0.0007 -0.0005]\n",
      " [-0.0047 -0.0034]]\n",
      "Input layer bias gradient: [-0.0125 -0.0024  0.0077  0.0063  0.0061 -0.003  -0.0111  0.0074 -0.0007\n",
      " -0.0051]\n",
      "Hidden layer 1 weight gradient: [[-0.0298 -0.0415 -0.0319 -0.0141 -0.0428 -0.0245 -0.0404 -0.0172 -0.0253\n",
      "  -0.0284]\n",
      " [ 0.0231  0.0321  0.0246  0.0109  0.0331  0.0189  0.0312  0.0133  0.0196\n",
      "   0.022 ]\n",
      " [ 0.0455  0.0634  0.0486  0.0215  0.0654  0.0373  0.0617  0.0262  0.0386\n",
      "   0.0434]\n",
      " [-0.0192 -0.0268 -0.0205 -0.0091 -0.0276 -0.0158 -0.026  -0.0111 -0.0163\n",
      "  -0.0183]\n",
      " [-0.035  -0.0487 -0.0374 -0.0165 -0.0502 -0.0287 -0.0474 -0.0202 -0.0296\n",
      "  -0.0333]\n",
      " [ 0.0205  0.0286  0.0219  0.0097  0.0295  0.0168  0.0278  0.0118  0.0174\n",
      "   0.0195]\n",
      " [-0.0215 -0.03   -0.023  -0.0102 -0.0309 -0.0176 -0.0291 -0.0124 -0.0182\n",
      "  -0.0205]\n",
      " [ 0.0147  0.0204  0.0157  0.0069  0.0211  0.012   0.0199  0.0085  0.0124\n",
      "   0.014 ]\n",
      " [-0.0367 -0.0511 -0.0392 -0.0173 -0.0527 -0.0301 -0.0497 -0.0212 -0.0311\n",
      "  -0.035 ]\n",
      " [-0.0248 -0.0345 -0.0265 -0.0117 -0.0355 -0.0203 -0.0335 -0.0143 -0.021\n",
      "  -0.0236]]\n",
      "Hidden layer 1 bias gradient: [-0.0553  0.0428  0.0844 -0.0356 -0.0648  0.038  -0.0399  0.0272 -0.068\n",
      " -0.0459]\n",
      "Hidden layer 2 weight gradient: [-0.6824 -0.5496 -0.709  -0.6587 -0.6588 -0.7589 -0.5797 -0.6423 -0.6978\n",
      " -0.7537]\n",
      "Hidden layer 2 bias gradient: [-1.3214]\n",
      "y prediction:[-0.0052]\n"
     ]
    }
   ],
   "source": [
    "print(\"Loss = \" + str(np.round(loss.item(),4)))\n",
    "print(\"Input layer weight gradient: \" + str(np.round(input_layer.weight.grad.tolist(),4)))\n",
    "print(\"Input layer bias gradient: \" + str(np.round(input_layer.bias.grad.tolist(),4)))\n",
    "print(\"Hidden layer 1 weight gradient: \" + str(np.round(hidden_layer1.weight.grad.tolist(),4)))\n",
    "print(\"Hidden layer 1 bias gradient: \" + str(np.round(hidden_layer1.bias.grad.tolist(),4)))\n",
    "print(\"Hidden layer 2 weight gradient: \" + str(np.round(hidden_layer2.weight.grad.tolist()[0],4)))\n",
    "print(\"Hidden layer 2 bias gradient: \" + str(np.round(hidden_layer2.bias.grad.tolist(),4)))\n",
    "print(\"y prediction:\" + str(np.round(y_pred.tolist(),4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7  Implement the forward propagation and backpropagation algorithm from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define weight and bias\n",
    "w1 = input_layer.weight.t().detach().numpy()\n",
    "w2 = hidden_layer1.weight.t().detach().numpy()\n",
    "w3 = hidden_layer2.weight.t().detach().numpy()\n",
    "b1 = input_layer.bias.detach().numpy()\n",
    "b2 = hidden_layer1.bias.detach().numpy()\n",
    "b3 = hidden_layer2.bias.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss = 0.4365\n",
      "Input layer weight gradient: [[-0.0116 -0.0084]\n",
      " [-0.0023 -0.0016]\n",
      " [ 0.0071  0.0052]\n",
      " [ 0.0058  0.0042]\n",
      " [ 0.0057  0.0041]\n",
      " [-0.0028 -0.002 ]\n",
      " [-0.0103 -0.0074]\n",
      " [ 0.0069  0.005 ]\n",
      " [-0.0007 -0.0005]\n",
      " [-0.0047 -0.0034]]\n",
      "Input layer bias gradient: [-0.0125 -0.0024  0.0077  0.0063  0.0061 -0.003  -0.0111  0.0074 -0.0007\n",
      " -0.0051]\n",
      "Hidden layer 1 weight gradient: [[-0.0298  0.0231  0.0455 -0.0192 -0.035   0.0205 -0.0215  0.0147 -0.0367\n",
      "  -0.0248]\n",
      " [-0.0415  0.0321  0.0634 -0.0268 -0.0487  0.0286 -0.03    0.0204 -0.0511\n",
      "  -0.0345]\n",
      " [-0.0319  0.0246  0.0486 -0.0205 -0.0374  0.0219 -0.023   0.0157 -0.0392\n",
      "  -0.0265]\n",
      " [-0.0141  0.0109  0.0215 -0.0091 -0.0165  0.0097 -0.0102  0.0069 -0.0173\n",
      "  -0.0117]\n",
      " [-0.0428  0.0331  0.0654 -0.0276 -0.0502  0.0295 -0.0309  0.0211 -0.0527\n",
      "  -0.0355]\n",
      " [-0.0245  0.0189  0.0374 -0.0158 -0.0287  0.0168 -0.0176  0.012  -0.0301\n",
      "  -0.0203]\n",
      " [-0.0404  0.0312  0.0617 -0.026  -0.0474  0.0278 -0.0291  0.0199 -0.0497\n",
      "  -0.0335]\n",
      " [-0.0172  0.0133  0.0262 -0.0111 -0.0202  0.0118 -0.0124  0.0085 -0.0212\n",
      "  -0.0143]\n",
      " [-0.0253  0.0196  0.0386 -0.0163 -0.0296  0.0174 -0.0182  0.0124 -0.0311\n",
      "  -0.021 ]\n",
      " [-0.0284  0.022   0.0434 -0.0183 -0.0333  0.0195 -0.0205  0.014  -0.035\n",
      "  -0.0236]]\n",
      "Hidden layer 1 bias gradient: [-0.0553  0.0428  0.0844 -0.0356 -0.0648  0.038  -0.0399  0.0272 -0.068\n",
      " -0.0459]\n",
      "Hidden layer 2 weight gradient: [-0.6824 -0.5496 -0.709  -0.6587 -0.6588 -0.7589 -0.5797 -0.6423 -0.6978\n",
      " -0.7537]\n",
      "Hidden layer 2 bias gradient: [-1.3214]\n",
      "y prediction: [-0.0052]\n"
     ]
    }
   ],
   "source": [
    "for t in range(1):\n",
    "    # Forward pass\n",
    "    h1 = x.dot(w1)+b1\n",
    "    h1_ = 1.0 / (1.0 + np.exp(-h1))\n",
    "    h2 = h1_.dot(w2)+b2\n",
    "    h2_ = 1.0 / (1.0 + np.exp(-h2))\n",
    "    y_pred_ = h2_.dot(w3)+b3\n",
    "    loss_ = np.square(y_pred_ - y).sum()\n",
    "    \n",
    "    # Backward pass\n",
    "    dy_pred = 2.0 * (y_pred_ - y)\n",
    "    \n",
    "    dt = float(dy_pred)\n",
    "    dw3 = np.dot(h2_.T, dt)\n",
    "    db3 = np.ones(1).dot(dt)\n",
    "    \n",
    "    dt = np.dot(dt, w3.T)*h2_*(1-h2_)\n",
    "    dw2 = np.dot(h1_.reshape(len(h1_),1), dt)\n",
    "    db2 = np.ones(1).dot(dt)\n",
    "    \n",
    "    dt = np.dot(dt, w2.T)*h1_*(1-h1_)\n",
    "    dw1 = np.dot(x.reshape(len(x),1), dt)\n",
    "    db1 = np.ones(1).dot(dt)\n",
    "    \n",
    "    w1 -= 0.1 * dw1 # learning rate is 0.1\n",
    "    w2 -= 0.1 * dw2\n",
    "    w3 -= 0.1 * dw3.reshape(len(dw3),1)\n",
    "    \n",
    "    b1 -= 0.1 * db1 \n",
    "    b2 -= 0.1 * db2\n",
    "    b3 -= 0.1 * db3\n",
    "    \n",
    "print(\"Loss = \" + str(round(loss_,4)))    \n",
    "print(\"Input layer weight gradient: \" + str(np.round(dw1.T,4)))\n",
    "print(\"Input layer bias gradient: \" +  str(np.round(db1,4)))\n",
    "print(\"Hidden layer 1 weight gradient: \" + str(np.round(dw2,4)))\n",
    "print(\"Hidden layer 1 bias gradient: \" + str(np.round(db2,4)))\n",
    "print(\"Hidden layer 2 weight gradient: \" + str(np.round(dw3,4)))   \n",
    "print(\"Hidden layer 2 bias gradient: \" + str(np.round(db3,4)))    \n",
    "print(\"y prediction: \" + str(np.round(y_pred_,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8. Numerical precision errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.round(loss.item(),4)\n",
    "A = round(loss_,4)\n",
    "\n",
    "b = np.round(input_layer.weight.grad.tolist(),4)\n",
    "B = np.round(dw1.T,4)\n",
    "\n",
    "c = np.round(input_layer.bias.grad.tolist(),4)\n",
    "C = np.round(db1,4)\n",
    "\n",
    "d = np.round(hidden_layer1.weight.grad.tolist(),4)\n",
    "D = np.round(dw2,4)\n",
    "\n",
    "e = np.round(hidden_layer1.bias.grad.tolist(),4)\n",
    "E = np.round(db2,4)\n",
    "\n",
    "f = np.round(hidden_layer2.weight.grad.tolist()[0],4)\n",
    "F = np.round(dw3,4)\n",
    "\n",
    "g = np.round(hidden_layer2.bias.grad.tolist(),4)\n",
    "G = np.round(db3,4)\n",
    "\n",
    "h = np.round(y_pred.detach().numpy())\n",
    "H = np.round(y_pred_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.6824 -0.5496 -0.709  -0.6587 -0.6588 -0.7589 -0.5797 -0.6423 -0.6978\n",
      " -0.7537] [-0.6824 -0.5496 -0.709  -0.6587 -0.6588 -0.7589 -0.5797 -0.6423 -0.6978\n",
      " -0.7537]\n"
     ]
    }
   ],
   "source": [
    "print(f,F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(a-A)\n",
    "print(mean_squared_error(b,B))\n",
    "print(mean_squared_error(c,C))\n",
    "print(mean_squared_error(d,D))\n",
    "print(mean_squared_error(e,E))\n",
    "print(mean_squared_error(f,F))\n",
    "print(mean_squared_error(g,G))\n",
    "print((h-H)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
